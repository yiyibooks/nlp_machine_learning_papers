# NLP 和机器学习论文中文翻译

在阅读过程中发现有翻译不好的地方，请直接修改

用户可自己直接输入 arXiv 上的论文地址上传想要阅读的论文

[https://www.yiyibooks.cn/](https://www.yiyibooks.cn/)

[Big Self-Supervised Models are Strong Semi-Supervised Learners, 2020](https://yiyibooks.cn/nlp/SimCLRv2/index.html), 图像领域无监督预训练 + 有监督微调 SimCLR v2

[ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS, 2020](https://yiyibooks.cn/nlp/ELECTRA/index.html), ELECTRA 生成器+判别器预训练语言模型

[DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue, 2020](https://yiyibooks.cn/nlp/DialoGLUE/index.html), DialoGLUE：用于面向任务对话的自然语言理解基准

[DIET: Lightweight Language Understanding for Dialogue Systems, 2020](https://yiyibooks.cn/nlp/diet/index.html), Rasa DIET：对话系统的轻量级语言理解

[ConveRT: Efficient and Accurate Conversational Representations from Transformers, 2020](https://yiyibooks.cn/nlp/ConvRT/index.html), ConveRT：基于 Transformer 的高效、准确的会话表示

[ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems, 2020](https://yiyibooks.cn/nlp/ConvLab2/index.html), 对话系统工具包 ConLab-2

[Poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring, 2020](https://yiyibooks.cn/nlp/poly-encoder/index.html), 多语句评分 poly-encoder

[FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence, 2020](https://yiyibooks.cn/nlp/FixMatch/index.html), 半监督学习 FixMatch

[XLNet: Generalized Autoregressive Pretraining for Language Understanding, 2019](https://yiyibooks.cn/nlp/XLNet/index.html), xlnet

[Unsupervised Data Augmentation for Consistency Training, 2019](https://yiyibooks.cn/nlp/uda/index.html), 无监督数据的增强 UDA

[EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks, 2019](https://yiyibooks.cn/nlp/EDA/index.html), 有监督数据的增强 EDA

[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, 2019](https://yiyibooks.cn/nlp/SentenceBERT_Sentence_Embeddings_using_Siamese_BERTNetworks/index.html), sentence-bert

[RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2019](https://yiyibooks.cn/nlp/roberta/index.html), 预训练方法优化的 BERT, Roberta

[Albert: A Lite Bert For Self-Supervised Learning Of Language Representations, 2019](https://yiyibooks.cn/yiyibooks/A_LITE_BERT_FOR_SELFSUPERVISED_LEARNING_OF_LANGUAGE_REPRESENTATIONS/index.html), 内存优化的 BERT，Albert

[An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction, 2019](https://yiyibooks.cn/yiyibooks/An_Evaluation_Dataset_for_Intent_Classification/index.html), Intent Classification dataset

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018](https://yiyibooks.cn/nlp/bert/main.html), bert

[WAV2LETTER++: THE FASTEST OPEN-SOURCE SPEECH RECOGNITION SYSTEM, 2018](https://yiyibooks.cn/nlp/wae2letter++/index.html), WAV2LETTER++：最快的开源语音识别系统

[Attention Is All You Need, 2017](https://yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html), Transformer

[Layer Normalization, 2016](https://yiyibooks.cn/nlp/layer_norm/index.html), 层归一化 Layer Normalization

[Gaussian Error Linear Units (GELUs), 2016](https://yiyibooks.cn/nlp/gelu/main.html), 非线性 gelu

[Neural Machine Translation of Rare Words with Subword Units, 2016](https://yiyibooks.cn/yiyibooks/Neural_Machine_Translation_of_Rare_Words_with_Subword_Units/index.html), 子词 subword, BPE

[Pointer Networks, 2015](https://yiyibooks.cn/nlp/pointer_network/index.html), 指针网络 pointer networks

[Effective Approaches to Attention-based Neural Machine Translation, 2015](https://yiyibooks.cn/yiyibooks/Effective_Approaches_to_Attention_Based_Neural_Machine_Translation/index.html)

[Distilling the Knowledge in a Neural Network, 2015](https://yiyibooks.cn/nlp/Knowledge_Distilling/index.html), 知识蒸馏 Knowledge distilling

[Neural Machine Translation by Jointly Learning to Align and Translate, 2014](https://yiyibooks.cn/yiyibooks/Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate/index.html), Seq2seq with attention

[Convolutional Neural Networks for Sentence Classification, 2014](https://yiyibooks.cn/nlp/textcnn/index.html), 文本分类 textcnn

[Sequence to Sequence Learning with Neural Networks](https://www.yiyibooks.cn/arxiv/1409.3215v3/index.html), 使用神经网络进行序列到序列学习 Seq2seq

[BLEU: a Method for Automatic Evaluation of Machine Translation, 2002](https://yiyibooks.cn/yiyibooks/BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation/index.html), 机器翻译评价指标 bleu
